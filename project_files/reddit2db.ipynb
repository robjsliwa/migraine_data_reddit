{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Reddis Posts in the Database\n",
    "\n",
    "This notebook uses pushshift.io APIs to store Reddis posts from\n",
    "migraine subreddit in the database.\n",
    "\n",
    "Accessing Reddis data via APIs is slow.  For example, pushshift.io has limits on how often given endpoint can be accessed.  In order to gather 10000 posts from pushshift.io it takes close to 8 hours since we needed to throttle our APIs to avoid `429 Too Many Requests` errors.  For this reason we chose to store data retrieved from pushshift.io in MongoDB.  MongoDB's document data model supports JSON and it has great query language.  Since pushshift.io already returns posts as JSON objects MongoDB is a great fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB Setup\n",
    "\n",
    "Rather than dealing with differences of MongoDB setup on different operating systems we decided to use Docker based approach.  Here is the `docker-compose.yml`:\n",
    "\n",
    "```\n",
    "version: \"3\"\n",
    "services:\n",
    "  mongo:\n",
    "    image: mongo:latest\n",
    "    volumes:\n",
    "      - ./dbdata:/data/db\n",
    "    ports:\n",
    "      - 37017:27017\n",
    "\n",
    "```\n",
    "\n",
    "You can start mongodb using `docker-compose up -d`.  Notice Mongodb folder `/data/db` is default location for database files and `volumes` entry maps it to host's `/dbdata`.  You can change this if you want to use different folder on your system.  Mapping to the host's folder allows restarts of the container without losing the data.  In addition, we can just zip `dbdata` folder to share database files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Database\n",
    "\n",
    "- Make sure you started mongodb with `docker-compose up -d`\n",
    "- mongodb running in the container is exposed on port 37017 to prevent confusion in case you already have mongo installed on your system and running on default port 27017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pprint\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'reddit-migraine'\n",
    "\n",
    "client = MongoClient('mongodb://localhost:37017')\n",
    "db = client[database_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Indexes\n",
    "\n",
    "You need to create indexes on the fields below so you can quickly access data when it grows big.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'author_1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_collection = 'posts'\n",
    "\n",
    "db[posts_collection].create_index('id', unique=True)\n",
    "db[posts_collection].create_index('selftext', unique=False)\n",
    "db[posts_collection].create_index('title', unique=False)\n",
    "db[posts_collection].create_index('author', unique=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Helper Methods\n",
    "\n",
    "These methods access Reddit posts and comments from pushshift.io server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(pushshift_url, subreddit_name, before_time, max_size=100):\n",
    "    should_retry = True\n",
    "    while should_retry:\n",
    "        try:\n",
    "            req = requests.get(f'{pushshift_url}/?subreddit={subreddit_name}&sort=desc&sort_type=created_utc&before={before_time}&size={max_entries}')\n",
    "            output = req.json()\n",
    "            should_retry = False\n",
    "        except:\n",
    "            print(f'retrying post...')\n",
    "            time.sleep(5)\n",
    "            should_retry = True\n",
    "    return output\n",
    "\n",
    "def get_comments(pushshift_url, comment_id, max_size=100):\n",
    "    should_retry = True\n",
    "    while should_retry:\n",
    "        try:\n",
    "            req = requests.get(f'{pushshift_url}/?link_id={comment_id}&limit={max_size}')\n",
    "            output = req.json()\n",
    "            should_retry = False\n",
    "        except:\n",
    "            print(f'retrying comment {comment_id}...')\n",
    "            time.sleep(5)\n",
    "            should_retry = True\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URLs and Constants\n",
    "\n",
    "Define needed constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushshift_post_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "pushshift_comment_url = 'https://api.pushshift.io/reddit/comment/search'\n",
    "subreddit_name = 'migraine'\n",
    "max_entries = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Posts\n",
    "\n",
    "Get posts and comments associated with each post.  For each post store the comments in new field `comments` as a mongodb embedded array.\n",
    "\n",
    "To start getting comments use `before_time` with current time as shown in the code below.  This will start with latest post and continue until it gets specified number of iterations or reads all of the available date from pushshift.io.\n",
    "\n",
    "Notice that each iteration retrieves `max_entries` as defined above in constants.\n",
    "\n",
    "After each iteration new `before_time` is printed out.  This is useful in case the code or website crashes and you want to start over.  This serves as checkpoint and you can take that new value and assign it to `before_time` variable and restart the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_time = int(time.time())  # current epoch time\n",
    "total_posts = 0\n",
    "\n",
    "def get_page_of_posts(before_time):\n",
    "    posts = get_posts(pushshift_post_url, subreddit_name, before_time, max_entries)\n",
    "    data = posts.get('data', [])\n",
    "\n",
    "    for entry in data:\n",
    "        comment_id = entry['id']\n",
    "        comments = get_comments(pushshift_comment_url, comment_id)\n",
    "        entry['comments'] = comments['data']\n",
    "        time.sleep(1)\n",
    "    return data\n",
    "\n",
    "for _ in range(10000):\n",
    "    posts = get_page_of_posts(before_time)\n",
    "    for post in posts:\n",
    "        db['posts'].insert_one(post)\n",
    "    total_posts += len(posts)\n",
    "    print(f'Inserted {len(posts)} posts... Total posts so far: {total_posts}')\n",
    "    print(f'Last before time: {before_time}')\n",
    "    before_time = posts[len(posts) - 1]['created_utc']\n",
    "    print(f'Next before time: {before_time}')\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f'Done.  Total new posts: {total_posts}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "043ded0200596b90969cdaf78db8cc3255494a426d99ef977fa69e36f146e9e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('migraine-data-reddit-WM5Df_6q-py3.9': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
